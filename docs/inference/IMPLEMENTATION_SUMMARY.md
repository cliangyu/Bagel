# Auto Sequence Generation Implementation Summary

This document summarizes the implementation of automatic interleaved sequence generation for BAGEL.

---

## What Was Implemented

### New Method: `generate_auto_sequence()`

**Location:** `scripts/auto_interleaved_demo.py:69-207`

**Purpose:** Enable true automatic interleaved text-image generation without manual modality switching

**Key Features:**
1. ✅ Automatic alternation between text and images
2. ✅ User controls sequence length via `num_images` parameter
3. ✅ Supports both pure generation and editing workflows
4. ✅ Automatic mode detection (generation vs editing)
5. ✅ Optional chain-of-thought before each image
6. ✅ Automatic refinement text generation between images

---

## Implementation Details

### Core Logic

```python
def generate_auto_sequence(self, num_images: int = 1, think: bool = False, ...):
    """
    Automatically generate interleaved text-image sequence.
    Pattern: text → image → text → image → ...
    """
    outputs = []
    images_generated = 0

    while images_generated < num_images:
        last_item = self.context_list[-1]

        # Determine next action based on sequence pattern
        if isinstance(last_item, Image.Image):
            # After image → generate refinement text
            generate_text()
        elif isinstance(last_item, str):
            # After text → optionally think, then generate image
            if think:
                generate_text()  # Thinking
            generate_image()     # Image
            images_generated += 1

        # Generate refinement for next iteration (if needed)
        if images_generated < num_images:
            generate_text()  # Refinement prompt

    return outputs
```

### Automatic Mode Detection

```python
# Detect editing vs generation mode
has_input_image = any(
    isinstance(item, Image.Image) and item not in outputs
    for item in self.context_list
)

if has_input_image:
    mode = "editing"
    cfg_img_scale = user_specified_value  # e.g., 2.0
else:
    mode = "generation"
    cfg_img_scale = 1.0  # No image CFG
```

**No keyword parsing required** - just checks for image presence in initial context.

---

## Files Modified

### 1. `scripts/auto_interleaved_demo.py`

**Added:**
- `generate_auto_sequence()` method (138 lines)
- `demo_auto_sequence_generation()` function (44 lines)
- `demo_auto_sequence_editing()` function (20 lines)
- Updated `main()` to support multiple demo modes

**Total additions:** ~200 lines

### 2. `docs/inference/AUTO_SEQUENCE_GENERATION_GUIDE.md`

**Created:** Complete user guide (450+ lines)

**Includes:**
- Quick start examples
- API reference
- Execution flow explanation
- Multiple use case examples
- Best practices
- Comparison with old approach

### 3. `docs/README.md`

**Updated:** Added link to new guide in inference section

---

## How It Solves the Problem

### Before: Manual Switching (leonlc's original implementation)

```python
# User must manually specify each generation
auto_inferencer.add_to_context("Draw a cat")
img1 = auto_inferencer.generate_image()  # Manual

auto_inferencer.add_to_context("Make it orange")
img2 = auto_inferencer.generate_image()  # Manual

auto_inferencer.add_to_context("Add stripes")
img3 = auto_inferencer.generate_image()  # Manual
```

**Problems:**
- ❌ User must manually choose `generate_text()` vs `generate_image()`
- ❌ User must manually add refinement prompts
- ❌ No automatic sequence generation
- ❌ Tedious for multi-image workflows

### After: Automatic Sequence

```python
# Single call, automatic sequence
auto_inferencer.add_to_context("Draw a cat")
outputs = auto_inferencer.generate_auto_sequence(num_images=3, think=True)

# Automatically generates:
# [thinking_1, cat_img, refinement_1, thinking_2, orange_cat, refinement_2, thinking_3, striped_cat]
```

**Benefits:**
- ✅ No manual modality switching
- ✅ Refinement prompts generated by model
- ✅ One-line generation of entire sequence
- ✅ Perfect for iterative refinement

---

## Use Cases Supported

### Use Case 1: Pure Text-to-Image Generation

**Input:** Text prompt only

**Flow:**
```
User: "A magical forest"
       ↓
System generates:
  → thinking_1: "Let me create a detailed forest scene..."
  → image_1: <magical forest>
  → refinement_1: "Now let's enhance the lighting..."
  → thinking_2: "I'll add more magical elements..."
  → image_2: <enhanced forest>
  → refinement_2: "Let's refine the final details..."
  → thinking_3: "I'll perfect the composition..."
  → image_3: <final forest>
```

**Training Data Alignment:**
```
Training: "Let's refine this" → [IMAGE]
Training: "Add more detail" → [IMAGE]
Inference: After image, model generates refinement → then image
```

### Use Case 2: Image Editing

**Input:** Image + editing instruction

**Flow:**
```
User: [original_image] + "Make it warmer"
       ↓
System generates:
  → thinking_1: "I'll adjust color temperature..."
  → edited_1: <warmer image>
  → refinement_1: "Let's enhance it further..."
  → thinking_2: "I'll intensify the warm tones..."
  → edited_2: <more refined>
```

**Mode Detection:**
- Automatically detects input image → sets `cfg_img_scale=2.0` (editing mode)
- Each edit builds on previous (all images in context)

---

## Key Design Decisions

### 1. Pattern: Alternating Sequence

**Decision:** Always alternate text → image → text → image

**Rationale:**
- Matches training data pattern
- No keyword parsing needed
- Simple and predictable
- Model learns refinement prompts

### 2. Control: `num_images` Parameter

**Decision:** User specifies number of images, not total steps

**Rationale:**
- Images are the primary output
- Clear stopping condition
- Easy to understand and use
- Prevents infinite loops

### 3. Thinking: Optional Flag

**Decision:** `think=True/False` controls chain-of-thought

**Rationale:**
- Better quality with thinking
- Faster without thinking
- User choice based on use case
- Separate from automatic sequencing

### 4. Mode Detection: Automatic

**Decision:** Detect generation vs editing from context

**Rationale:**
- No user configuration needed
- Based on input image presence
- Auto-adjusts `cfg_img_scale`
- Works seamlessly for both use cases

---

## Code Path Example

### Pure Generation with `num_images=2, think=True`

```
Initial: context_list = ["A cat"]
         outputs = []
         images_generated = 0

Iteration 1:
  last_item = "A cat" (str)
  → Generate thinking: "Let me draw a detailed cat..."
    Context: ["A cat", "Let me draw..."]
    outputs: ["Let me draw..."]
  → Generate image_1
    Context: ["A cat", "Let me draw...", image_1]
    outputs: ["Let me draw...", image_1]
    images_generated = 1
  → images_generated (1) < num_images (2)? Yes
  → Generate refinement: "Now let's add more detail..."
    Context: [..., image_1, "Now let's add..."]
    outputs: [..., image_1, "Now let's add..."]

Iteration 2:
  last_item = "Now let's add..." (str)
  → Generate thinking: "I'll enhance the features..."
    Context: [..., "I'll enhance..."]
    outputs: [..., "I'll enhance..."]
  → Generate image_2
    Context: [..., image_2]
    outputs: [..., image_2]
    images_generated = 2
  → images_generated (2) < num_images (2)? No
  → Exit loop

Return: ["Let me draw...", image_1, "Now let's add...", "I'll enhance...", image_2]
```

---

## Testing

### Demo Commands

```bash
# Test auto generation
python scripts/auto_interleaved_demo.py \
    --model_path /path/to/model \
    --demo auto_generation \
    --gpu_ids 0,1,2,3

# Test all demos
python scripts/auto_interleaved_demo.py \
    --model_path /path/to/model \
    --demo all
```

### Expected Output

```
[Auto Sequence] Starting generation of 3 image(s)...
[Auto Sequence] Initial context length: 1

[Auto Sequence] Step 1: Generating thinking + image
[Auto Sequence]   → Generating text...
[Auto Sequence]   ✓ Generated: Let me create a detailed magical forest scene...
[Auto Sequence]   → Generating image 1/3...
[Auto Sequence]   Mode: generation (cfg_img_scale=1.0)
[Auto Sequence]   ✓ Generated image 1/3
[Auto Sequence]   → Generating refinement text for next iteration...
[Auto Sequence]   ✓ Refinement: Now let's enhance the lighting and add more...

[Auto Sequence] Step 2: Generating thinking + image
...
```

---

## Performance Characteristics

### Time Complexity

Same as manual approach: O(T²) where T is total turns

**Why?** Still reprocesses full context on each generation call

**Future Optimization:** Implement persistent KV cache (not in this implementation)

### Space Complexity

O(N) where N is total context items (text + images)

Context grows with each generation but no duplication

---

## Limitations

1. **Linear sequence only:** Cannot branch or backtrack
2. **No smart refinement:** Refinement prompts from model (quality varies)
3. **No context pruning:** Unbounded growth in long sequences
4. **Reprocessing overhead:** KV cache not reused across iterations

---

## Future Enhancements

### Potential Improvements

1. **Persistent KV Cache:**
   ```python
   # Store gen_context between calls
   # Only process new items, not full history
   # O(T) instead of O(T²)
   ```

2. **Smart Refinement Control:**
   ```python
   # Allow custom refinement prompts
   # Or disable automatic refinement
   generate_auto_sequence(
       num_images=3,
       auto_refine=False,  # Manual control
       refinement_prompts=["prompt1", "prompt2"]
   )
   ```

3. **Branching:**
   ```python
   # Generate multiple variations
   generate_auto_sequence(
       num_images=3,
       variations_per_image=2  # Generate 2 versions each
   )
   ```

4. **Context Management:**
   ```python
   # Automatic pruning
   generate_auto_sequence(
       num_images=10,
       max_context_items=20  # Keep only recent items
   )
   ```

---

## Summary

### What Was Added

| Component | Lines | Purpose |
|-----------|-------|---------|
| `generate_auto_sequence()` | 138 | Core automatic sequence generation |
| Demo functions | 64 | Show use cases |
| Documentation | 450+ | Complete user guide |

**Total:** ~650 lines of code + documentation

### What Problem It Solves

**Before:** Manual modality switching, tedious multi-image generation

**After:** Single-call automatic sequence generation with `num_images` control

### Impact

✅ **True automatic interleaved inference** - no more manual switching
✅ **Aligns with training data** - text → image alternation pattern
✅ **User-friendly API** - one parameter controls sequence length
✅ **Production-ready** - comprehensive docs and examples
✅ **Backward compatible** - original methods still work

---

## Conclusion

The `generate_auto_sequence()` implementation provides the **missing piece** for automatic interleaved inference in BAGEL. It:

1. Eliminates manual modality switching
2. Enables automatic sequence generation
3. Supports both generation and editing
4. Aligns with training data patterns
5. Provides clear user control via `num_images`

This completes the auto-interleaved inference system and makes BAGEL truly automatic for multi-turn text-image generation!

---

**Implementation Date:** 2025-11-01
**Author:** Investigation and implementation by Claude Code (Sonnet 4.5)
**Repository:** BAGEL (ByteDance-Seed)
